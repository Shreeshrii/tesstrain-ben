export

# Disable built-in suffix rules.
# This makes starting with a very large number of GT lines much faster.
.SUFFIXES:

## Make sure that sort always uses the same sort order.
LC_ALL := C

SHELL := /bin/bash
LOCAL := $(PWD)/usr
PATH := $(LOCAL)/bin:$(PATH)

# Path to the .traineddata directory with traineddata suitable for training 
# (for example from tesseract-ocr/tessdata_best). Default: $(LOCAL)/share/tessdata
TESSDATA =  $(LOCAL)/share/tessdata

# Name of the model to be built. Default: $(MODEL_NAME)
MODEL_NAME = foo

# Data directory for output files, proto model, start model, etc. Default: $(DATA_DIR)
DATA_DIR = data

# Output directory for generated files. Default: $(OUTPUT_DIR)
OUTPUT_DIR = $(DATA_DIR)/$(MODEL_NAME)

# Name of the model to continue from. Default: '$(START_MODEL)'
START_MODEL =

LAST_CHECKPOINT = $(OUTPUT_DIR)/checkpoints/$(MODEL_NAME)_checkpoint

# Name of the proto model. Default: '$(PROTO_MODEL)'
PROTO_MODEL = $(OUTPUT_DIR)/$(MODEL_NAME).traineddata

# Ground truth directory. Default: $(GROUND_TRUTH_DIR)
GROUND_TRUTH_DIR := $(OUTPUT_DIR)-ground-truth

# If EPOCHS is given, it is used to set MAX_ITERATIONS.
ifeq ($(EPOCHS),)
# Max iterations. Default: $(MAX_ITERATIONS)
MAX_ITERATIONS := 10000
else
MAX_ITERATIONS := -$(EPOCHS)
endif

# Debug Interval. Default:  $(DEBUG_INTERVAL)
DEBUG_INTERVAL := 0

# Learning rate. Default: $(LEARNING_RATE)
ifdef START_MODEL
LEARNING_RATE := 0.0001
else
LEARNING_RATE := 0.002
endif

# Replace top layer of network, Finetune, from Scratch training
ifeq ($(TRAIN_TYPE),FineTune)
	NET_SPEC =--continue_from $(DATA_DIR)/$(START_MODEL)/$(MODEL_NAME).lstm --old_traineddata $(TESSDATA)/$(START_MODEL).traineddata 
else
ifeq ($(TRAIN_TYPE),ReplaceLayer)
	NET_SPEC =--continue_from $(DATA_DIR)/$(START_MODEL)/$(MODEL_NAME).lstm --append_index 5 --net_spec '[Lfx192O1c1]'
else
	NET_SPEC =--net_spec [1,36,0,1 Ct3,3,16 Mp3,3 Lfys48 Lfx96 Lrx96 Lfx192O1c1]
endif
endif

# Ratio of train / eval training data. Default: $(RATIO_TRAIN)
RATIO_TRAIN := 0.90

# Default Target Error Rate. Default: $(TARGET_ERROR_RATE)
TARGET_ERROR_RATE := 0.01

# Default Fonts Directory. Default: $(TESSTRAIN_FONTS_DIR)
TESSTRAIN_FONTS_DIR := /usr/share/fonts/

# Default Training Text. Default: $(TESSTRAIN_TEXT)
TESSTRAIN_TEXT := $(DATA_DIR)/$START_MODEL.training_text

# Font for training. Default: $(TESSTRAIN_FONT)
TESSTRAIN_FONT =
# Font List for training. Default: $(TESSTRAIN_FONT_LIST)
ifdef TESSTRAIN_FONT
TESSTRAIN_FONT_LIST =--fontlist $(TESSTRAIN_FONT)
else
TESSTRAIN_FONT_LIST =
endif

# Default maximum number of lines from training text. Default: $(TESSTRAIN_MAX_LINES)
TESSTRAIN_MAX_LINES := 10000

# Default Script for training. Default: $(TESSTRAIN_SCRIPT)
TESSTRAIN_SCRIPT := Latin

# Default Language for training. Default: $(TESSTRAIN_LANG)
TESSTRAIN_LANG := eng

# BEGIN-EVAL makefile-parser --make-help Makefile

help:
	@echo ""
	@echo "  Targets"
	@echo ""
	@echo "    lists                  Create lists of lstmf filenames for training and eval"
	@echo "    training               Start training"
	@echo "    traineddata            Create best and fast .traineddata files from each .checkpoint file"
	@echo "    proto-model            Build the proto model"
	@echo "    clean-log              Clean log file"
	@echo "    clean-groundtruth      Clean generated groundtruth files"
	@echo "    clean-output           Clean generated output files"
	@echo "    clean                  Clean all generated files"
	@echo ""
	@echo "  Variables"
	@echo ""
	@echo "    TESSDATA              Path to the .traineddata directory with traineddata suitable for training "
	@echo "                          (for example from tesseract-ocr/tessdata_best). Default: $(LOCAL)/share/tessdata"
	@echo "    MODEL_NAME            Name of the model to be built. Default: $(MODEL_NAME)"
	@echo "    OUTPUT_DIR            Output directory for generated files. Default: $(OUTPUT_DIR)"
	@echo "    START_MODEL           Name of the model to continue from. Default: '$(START_MODEL)'"
	@echo "    PROTO_MODEL           Name of the proto model. Default: '$(PROTO_MODEL)'"
	@echo "    GROUND_TRUTH_DIR      Ground truth directory. Default: $(GROUND_TRUTH_DIR)"
	@echo "    MAX_ITERATIONS        Max iterations. Default: $(MAX_ITERATIONS)"
	@echo "    EPOCHS                Set max iterations based on the number of lines for the training. Default: none"
	@echo "    DEBUG_INTERVAL        Debug Interval. Default:  $(DEBUG_INTERVAL)"
	@echo "    LEARNING_RATE         Learning rate. Default: $(LEARNING_RATE)"
	@echo "    RATIO_TRAIN           Ratio of train / eval training data. Default: $(RATIO_TRAIN)"
	@echo "    TARGET_ERROR_RATE     Target Error Rate. Default: $(TARGET_ERROR_RATE)"
	@echo "    TESSTRAIN_FONTS_DIR   Fonts Directory. Default: $(TESSTRAIN_FONTS_DIR)"
	@echo "    TESSTRAIN_TEXT        Training Text. Default: $(TESSTRAIN_TEXT)"
	@echo "    TESSTRAIN_FONT        Font for training. Default: $(TESSTRAIN_FONT)"
	@echo "    TESSTRAIN_MAX_LINES   Maximum number of lines from training text. Default: $(TESSTRAIN_MAX_LINES)"
	@echo "    TESSTRAIN_LANG        Language code of existing language for creating PROTO_MODEL. "
	@echo "                          (It can be the same as START_MODEL for fine-tuning). Default: $(TESSTRAIN_LANG)"
	@echo "    TESSTRAIN_SCRIPT      Language Script (eg. Latin for eng, Bengali for ben). Default: $(TESSTRAIN_SCRIPT)"
	@echo "    TRAIN_TYPE            Training Type - FineTune, ReplaceLayer or blank (from scratch). Default: '$(TRAIN_TYPE)'"
	
# END-EVAL

.PRECIOUS: $(OUTPUT_DIR)/checkpoints/$(MODEL_NAME)*_checkpoint

.PHONY: clean help proto_model lists training traineddata

ALL_LSTMF = $(OUTPUT_DIR)/all-lstmf

# Create lists of lstmf filenames for training and eval
lists: $(OUTPUT_DIR)/list.train $(OUTPUT_DIR)/list.eval

$(OUTPUT_DIR)/list.eval \
$(OUTPUT_DIR)/list.train: $(ALL_LSTMF)
	@mkdir -p $(OUTPUT_DIR)
	@total=$$(wc -l < $(ALL_LSTMF)); \
	  train=$$(echo "$$total * $(RATIO_TRAIN) / 1" | bc); \
	  test "$$train" = "0" && \
	    echo "Error: missing ground truth for training" && exit 1; \
	  eval=$$(echo "$$total - $$train" | bc); \
	  test "$$eval" = "0" && \
	    echo "Error: missing ground truth for evaluation" && exit 1; \
	  set -x; \
	  head -n "$$train" $(ALL_LSTMF) > "$(OUTPUT_DIR)/list.train"; \
	  tail -n "$$eval" $(ALL_LSTMF) > "$(OUTPUT_DIR)/list.eval"

# Start training
training: $(OUTPUT_DIR).traineddata

$(OUTPUT_DIR).traineddata: $(LAST_CHECKPOINT)
	lstmtraining \
	--stop_training \
	--continue_from $(LAST_CHECKPOINT) \
	--traineddata $(PROTO_MODEL) \
	--model_output $@

$(LAST_CHECKPOINT): proto_model lists
ifdef START_MODEL
	  @mkdir -p $(DATA_DIR)/$(START_MODEL)
	  combine_tessdata -e $(TESSDATA)/$(START_MODEL).traineddata  $(DATA_DIR)/$(START_MODEL)/$(MODEL_NAME).lstm
endif
	@mkdir -p $(OUTPUT_DIR)/checkpoints 
	lstmtraining \
	  $(NET_SPEC) \
	  --traineddata $(PROTO_MODEL) \
	  --train_listfile $(OUTPUT_DIR)/list.train \
	  --eval_listfile $(OUTPUT_DIR)/list.eval \
	  --max_iterations $(MAX_ITERATIONS) \
	  --debug_interval $(DEBUG_INTERVAL) \
	  --learning_rate $(LEARNING_RATE) \
	  --target_error_rate $(TARGET_ERROR_RATE) \
	  --model_output $(OUTPUT_DIR)/checkpoints/$(MODEL_NAME) \
	  > $(DATA_DIR)/$(MODEL_NAME).log 2>&1

proto_model: $(PROTO_MODEL) 

$(OUTPUT_DIR)/all-lstmf $(PROTO_MODEL): $(DATA_DIR)/$(TESSTRAIN_SCRIPT).unicharset
	python ./src/training/tesstrain.py \
	 --fonts_dir $(TESSTRAIN_FONTS_DIR) \
	 $(TESSTRAIN_FONT_LIST) \
	 --maxpages $(TESSTRAIN_MAX_LINES) \
	 --lang $(TESSTRAIN_LANG) \
	 --langdata_dir $(DATA_DIR) \
	 --training_text $(TESSTRAIN_TEXT) \
	 --tessdata_dir $(TESSDATA) \
	 --linedata_only --noextract_font_properties \
	 --exposures "0" --save_box_tiff  \
	 --output_dir $(DATA_DIR)/$(MODEL_NAME)-ground-truth
	mkdir -p $(DATA_DIR)/$(MODEL_NAME)
	mv -v $(DATA_DIR)/$(MODEL_NAME)-ground-truth/$(TESSTRAIN_LANG).training_files.txt $(DATA_DIR)/$(MODEL_NAME)/all-lstmf
	mv -v $(DATA_DIR)/$(MODEL_NAME)-ground-truth/$(TESSTRAIN_LANG)/$(TESSTRAIN_LANG).* $(DATA_DIR)/$(MODEL_NAME)/
	rename "s/$(TESSTRAIN_LANG)\./$(MODEL_NAME)\./g" $(DATA_DIR)/$(MODEL_NAME)/*.*
	bash box2gt.sh $(MODEL_NAME)

$(DATA_DIR)/$(TESSTRAIN_SCRIPT).unicharset: $(DATA_DIR)/Latin.unicharset $(DATA_DIR)/radical-stroke.txt
	wget -O $@ 'https://github.com/tesseract-ocr/langdata_lstm/raw/master/$(TESSTRAIN_SCRIPT).unicharset'
	
$(DATA_DIR)/Latin.unicharset:
	wget -O $@ 'https://github.com/tesseract-ocr/langdata_lstm/raw/master/Latin.unicharset'

$(DATA_DIR)/radical-stroke.txt:
	wget -O$@ 'https://github.com/tesseract-ocr/langdata_lstm/raw/master/radical-stroke.txt'

# Clean generated output files

clean-log:
	rm -rf $(DATA_DIR)/$(MODEL_NAME).log
	
clean-groundtruth:
	rm -rf $(GROUND_TRUTH_DIR)
	
.PHONY: clean-output
clean-output:
	rm -rf $(OUTPUT_DIR)

# Clean all generated files
clean: clean-output
